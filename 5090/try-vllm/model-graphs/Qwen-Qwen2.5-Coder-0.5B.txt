==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Qwen2Model                               --                        --
├─Embedding: 1-1                         [1, 32, 896]              136,134,656
├─Qwen2RotaryEmbedding: 1-2              [1, 32, 64]               --
├─ModuleList: 1-3                        --                        --
│    └─Qwen2DecoderLayer: 2-1            [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-1            [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-2          [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-3            [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-4                [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-2            [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-5            [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-6          [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-7            [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-8                [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-3            [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-9            [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-10         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-11           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-12               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-4            [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-13           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-14         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-15           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-16               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-5            [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-17           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-18         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-19           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-20               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-6            [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-21           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-22         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-23           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-24               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-7            [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-25           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-26         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-27           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-28               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-8            [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-29           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-30         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-31           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-32               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-9            [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-33           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-34         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-35           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-36               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-10           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-37           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-38         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-39           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-40               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-11           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-41           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-42         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-43           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-44               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-12           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-45           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-46         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-47           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-48               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-13           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-49           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-50         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-51           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-52               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-14           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-53           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-54         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-55           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-56               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-15           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-57           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-58         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-59           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-60               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-16           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-61           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-62         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-63           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-64               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-17           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-65           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-66         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-67           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-68               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-18           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-69           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-70         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-71           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-72               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-19           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-73           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-74         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-75           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-76               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-20           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-77           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-78         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-79           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-80               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-21           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-81           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-82         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-83           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-84               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-22           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-85           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-86         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-87           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-88               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-23           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-89           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-90         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-91           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-92               [1, 32, 896]              13,074,432
│    └─Qwen2DecoderLayer: 2-24           [1, 32, 896]              --
│    │    └─Qwen2RMSNorm: 3-93           [1, 32, 896]              896
│    │    └─Qwen2Attention: 3-94         [1, 32, 896]              1,836,160
│    │    └─Qwen2RMSNorm: 3-95           [1, 32, 896]              896
│    │    └─Qwen2MLP: 3-96               [1, 32, 896]              13,074,432
├─Qwen2RMSNorm: 1-4                      [1, 32, 896]              896
==========================================================================================
Total params: 494,032,768
Trainable params: 494,032,768
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 494.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 89.33
Params size (MB): 1976.13
Estimated Total Size (MB): 2065.46
==========================================================================================