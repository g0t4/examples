services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    # command: ollama run llama3 ?
# TODO amd:
#   docker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm
# or NVIDIA: https://hub.docker.com/r/ollama/ollama

volumes:
  ollama:

# docker compose exec ollama bash
#   ollama pull llama3
#   