from logging import logProcesses
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch, math
import io

def read_text(path, encoding="utf-8") -> str:
    with io.open(path, "r", encoding=encoding) as f:
        return f.read()

device = torch.device("mps")
# model = "Qwen/Qwen2.5-0.5B"  # * non coder variant! useful to see prediction differences (i.e. perplexity diffs!)
model = "Qwen/Qwen2.5-Coder-0.5B"
tok = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.float16).eval()
model.to(device)  # type: ignore

# %%

from indent_print import IndentPrinter

def perplexity_from_logits(code):
    ip = IndentPrinter()
    print = ip.print
    inputs = tok(code, return_tensors="pt").to(device)
    print(f'{inputs=}')
    ip.increment()
    for i in inputs.input_ids[0]:
        print(i)
        print(tok.decode(i))
    ip.decrement()
    print("\nout:")
    with torch.no_grad():
        out = model(**inputs)
        print(f'{out=}')
    ip.increment()
    total_perplex = 0
    for idx, logits in enumerate(out.logits[0]):
        # * input
        print(f'\n{idx}')
        input_token = inputs.input_ids[0][idx].item()
        input_txt = tok.decode(input_token)
        input_value = logits[input_token].item()
        ip.increment()
        print(f'{input_token=} {input_value=} "{input_txt}"')
        #
        # * pred next token (highest probability)
        max_idx = torch.argmax(logits).item()
        max_value = logits[max_idx].item()
        max_pred = tok.decode(max_idx)
        print(f'{max_idx=} {max_value=} "{max_pred}"')
        #
        # * normalize values
        min_value = torch.min(logits)
        print(f'{min_value=}')
        # norm_logits = (logits - min_value) / (max_value - min_value)
        shifted_to_zero_max = logits - max_value
        print(f'{torch.min(shifted_to_zero_max)=}')  #
        print(f'{torch.max(shifted_to_zero_max)=}')  #
        #
        # * exp e^x ... collapses all values from 0 to 1 (NOT NORMALIZED YET, but closer!)
        exp = torch.exp(shifted_to_zero_max)
        print(f'{torch.min(exp)=}')  # 0 min (as x=>-infinity, y=>0... IOTW e^-infinity = 0)
        print(f'{torch.max(exp)=}')  # 1 max (e^0 = 1)
        exp_sum = torch.sum(exp)
        print(f'{exp_sum=}')  # obviously not 1 yet!
        softmax = exp / exp_sum
        # * softmax normalization complete:
        print(f'{torch.min(softmax)=}')  # min >= 0
        print(f'{torch.max(softmax)=}')  # max <= 1 - hints at perplexity, max closer to 1 => high confidence (low perplexity), small max (i.e. 0.1) => low confidence (high perplexity)
        print(f'{torch.sum(softmax)=}')  # s/b 1 (normalized)
        #
        #
        # ENSURE sm == softmax (approx)
        sm = torch.softmax(logits, dim=-1)
        print(f'  {torch.min(sm)=}')  # min >= 0
        print(f'  {torch.max(sm)=}')  # max <= 1 - hints at perplexity, max closer to 1 => high confidence (low perplexity), small max (i.e. 0.1) => low confidence (high perplexity)
        print(f'  {torch.sum(sm)=}')  # s/b 1 (normalized)
        #
        log_softmax = torch.log(softmax)
        print(f'{torch.min(log_softmax)=}')  # min >= 0
        print(f'{torch.max(log_softmax)=}')  # max <= 1 - hints at perplexity, max closer to 1 => high confidence (low perplexity), small max (i.e. 0.1) => low confidence (high perplexity)
        print(f'{torch.sum(log_softmax)=}')  # s/b 1 (normalized)
        log_sm = torch.log(sm)
        print(f'  {torch.min(log_sm)=}')  # min >= 0
        print(f'  {torch.max(log_sm)=}')  # max <= 1 - hints at perplexity, max closer to 1 => high confidence (low perplexity), small max (i.e. 0.1) => low confidence (high perplexity)
        print(f'  {torch.sum(log_sm)=}')  # s/b 1 (normalized)
        #
        # FYI log_softmax AIO helps avoid rounding errors that lead to -infinity on lowest probabilit(ies)
        log_sm_aio = torch.nn.functional.log_softmax(logits, dim=-1)
        print(f'  {torch.min(log_sm_aio)=}')
        print(f'  {torch.max(log_sm_aio)=}')
        print(f'  {torch.sum(log_sm_aio)=}')
        #
        exp_log_probs = torch.exp(log_softmax)
        print(f'{torch.min(exp_log_probs)=}')
        print(f'{torch.max(exp_log_probs)=}')
        print(f'{torch.sum(exp_log_probs)=}')
        #
        # * actual next token
        if idx < len(inputs.input_ids[0]) - 1:
            actual_next_token = inputs.input_ids[0][idx + 1]
            print(f'{actual_next_token=}')
            prob_actual_next_token = log_sm_aio[actual_next_token]
            neg_prob_of_actual_next_token = -prob_actual_next_token
            print(f'{neg_prob_of_actual_next_token=}')
            total_perplex += neg_prob_of_actual_next_token
            print(f'{total_perplex=} (avg: {total_perplex/(idx+1)})')
        #
        #
        ip.decrement()
    ip.decrement()

#
simple = "local request = LastRequest:new(body)"  # 6.49357 (using loss directly)
perplexity_from_logits(simple)  # 6.49609375 (avg)

# %%

def line_isolated_perplexity(lines: list[str]):
    line_losses = []
    # PRN? SKIP COMMENTS!?
    for line in lines:
        print(line)
        line = line.strip()
        if not line:
            # skip empty and comments
            line_losses.append(None)
            continue
        is_comment = line.startswith("--") or line.startswith("#")
        if is_comment:
            # PRN decide if skip comments?
            line_losses.append(None)
            continue
        # TODO tokenize all prior lines too and just grab the token losses line by line... so,  one forward pass where prior lines factor into loss/perplexity but we only consider each line's loss
        inputs = tok(line, return_tensors="pt").to(device)
        with torch.no_grad():
            out = model(**inputs, labels=inputs.input_ids)
        if torch.isfinite(out.loss):
            line_losses.append(out.loss.item())
            continue
        line_losses.append(None)
    return line_losses

print(f"simple line isolated perplex: {line_isolated_perplexity([simple])}")  # 6.49357

# %%
def one_pass_linewise_perplexity(lines: list[str]):
    # Join all lines into one text block so the model sees full context.
    joined = "\n".join(lines)
    enc = tok(joined, return_tensors="pt").to(device)

    with torch.no_grad():
        out = model(**enc, labels=enc.input_ids)

    # Cross-entropy loss per token
    token_losses = out.loss.detach().new_zeros(enc.input_ids.shape)
    logits = out.logits[:, :-1]
    labels = enc.input_ids[:, 1:]
    per_token_loss = torch.nn.functional.cross_entropy(
        logits.reshape(-1, logits.size(-1)),
        labels.reshape(-1),
        reduction="none",
    ).reshape(labels.shape)

    # Map token losses back to lines
    all_text = tok.batch_decode(enc.input_ids[0], skip_special_tokens=True)
    text = "".join(all_text)
    line_breaks = [i for i, ch in enumerate(text) if ch == "\n"]

    line_losses, start = [], 0
    for br in line_breaks + [len(text)]:
        segment = text[start:br]
        if not segment.strip() or segment.lstrip().startswith(("--", "#")):
            line_losses.append(None)
        else:
            # average token losses over the segmentâ€™s tokens
            segment_ids = tok(segment, return_tensors="pt").to(device).input_ids[0]
            mask = torch.isin(enc.input_ids[0], segment_ids)
            seg_loss = per_token_loss[0][mask[1:]]  # shift by 1 for next-token prediction
            line_losses.append(seg_loss.mean().item() if seg_loss.numel() else None)
        start = br + 1

    return line_losses

print(f"simple onepass perplexity: {one_pass_linewise_perplexity([simple])}")  # 6.49357

# %%
import rich

func1 = read_text("func1.lua")

lines = func1.split("\n")
line_perplexities = one_pass_linewise_perplexity(lines)
# line_perplexities = line_isolated_perplexity(lines)
print(f'{line_perplexities=}')

perplexities_skip_none = [p for p in line_perplexities if p is not None]
print(f'{perplexities_skip_none}')
perplexities_tensor = torch.tensor(perplexities_skip_none)
lines_mean = torch.mean(perplexities_tensor)
lines_std = torch.std(perplexities_tensor)
rich.print(f'{lines_mean=} {lines_std=}')

alpha_spike_factor = 2.0
for idx, line in enumerate(lines):
    perplex = line_perplexities[idx]
    threshold = alpha_spike_factor * lines_std + lines_mean
    if perplex is not None and perplex > threshold:
        line = "*" + line[1:]
        rich.print(f"[red]{idx+1}: {line}         {perplex}[/red]")
    else:
        rich.print(f"{idx+1}: {line}        {perplex}")
